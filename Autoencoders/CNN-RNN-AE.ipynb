{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import skimage.io\n",
    "import skimage.color\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import read_cifar10 as cf10\n",
    "\n",
    "#@read_data.restartable\n",
    "def cifar10_dataset_generator(dataset_name, batch_size, restrict_size=1000):\n",
    "    assert dataset_name in ['train', 'test']\n",
    "    assert batch_size > 0 or batch_size == -1  # -1 for entire dataset\n",
    "    \n",
    "    X_all_unrestricted, y_all = (cf10.load_training_data() if dataset_name == 'train'\n",
    "                                 else cf10.load_test_data())\n",
    "    \n",
    "    actual_restrict_size = restrict_size if dataset_name == 'train' else int(1e10)\n",
    "    X_all = X_all_unrestricted[:actual_restrict_size]\n",
    "    data_len = X_all.shape[0]\n",
    "    batch_size = batch_size if batch_size > 0 else data_len\n",
    "    \n",
    "    X_all_padded = np.concatenate([X_all, X_all[:batch_size]], axis=0)\n",
    "    y_all_padded = np.concatenate([y_all, y_all[:batch_size]], axis=0)\n",
    "    \n",
    "    for slice_i in range(math.ceil(data_len / batch_size)):\n",
    "        idx = slice_i * batch_size\n",
    "        #X_batch = X_all_padded[idx:idx + batch_size]\n",
    "        X_batch = X_all_padded[idx:idx + batch_size]*255  # bugfix: thanks Zezhou Sun!\n",
    "        y_batch = np.ravel(y_all_padded[idx:idx + batch_size])\n",
    "        yield X_batch.astype(np.uint8), y_batch.astype(np.uint8)\n",
    "\n",
    "cifar10_dataset_generators = {\n",
    "    'train': cifar10_dataset_generator('train', 1000),\n",
    "    'test': cifar10_dataset_generator('test', -1)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load cifar-10 data\n",
    "cf10_tr=cf10.load_training_data()\n",
    "cf10_tr_img=cf10_tr[0]\n",
    "cf10_tr_label = cf10_tr[1]\n",
    "\n",
    "cf10_test=cf10.load_test_data()\n",
    "cf10_test_img=cf10_test[0]\n",
    "cf10_test_label = cf10_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import skimage.io\n",
    "def img2block(im):\n",
    "    #lion[:2816,:4224,:]\n",
    "    #skimage.io.imshow(lion)\n",
    "    #skimage.io.show()\n",
    "    \n",
    "    im = im.astype(np.float32)\n",
    "    row,col,color = im.shape\n",
    "    im_bl=np.zeros((int(row*col/1024),32,32,3)).astype(np.float32)\n",
    "    count=0\n",
    "    for i in range(0,row-row%32,32):\n",
    "        for j in range(0,col-col%32,32):\n",
    "            im_bl[count,:,:,:]=im[i:i+32,j:j+32,:]\n",
    "            count = count +1\n",
    "    im_bl=im_bl/255.\n",
    "    return im_bl\n",
    "\n",
    "def block2img(img_blocks,img_size):\n",
    "    \n",
    "    row,col = img_size\n",
    "    img=np.zeros((row,col,3)).astype(np.float32)\n",
    "    n,k,l,c=img_blocks.shape\n",
    "                 \n",
    "    for i in range(0,int(row/k)):\n",
    "        for j in range(0,int(col/k)):\n",
    "            img[i*k:(i+1)*k,j*l:(j+1)*l,:]=img_blocks[int(i*col/k+j),:,:,:]\n",
    "    return img\n",
    "\n",
    "#Get the patches of lena image\n",
    "lena_img = skimage.io.imread('../test_img/lena512color.tiff')\n",
    "lena_32=img2block(lena_img)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function for converting a double to uint8\n",
    "def convert2uint8(img):\n",
    "    img[img>255]=255\n",
    "    img[img<0]=0\n",
    "    return img.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-2 - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create the inputs in the desired format\n",
    "x_tr = cf10_tr_img.astype(np.float32)#*255.\n",
    "x_test = cf10_test_img.astype(np.float32)#*255.\n",
    "x_test=x_test[:200,:,:,:]\n",
    "img = skimage.io.imread('../test_img/lena512color.tiff')\n",
    "img_32=img2block(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cnn_autoencoder(x_,kernels1=[5,7],kernels2=[7,5],filters1=[16,128],filters2=[128,3],pool_size=[1,2,2,1],name='autoencoder'):\n",
    "    '''\n",
    "    Autoencoder network\n",
    "    \n",
    "    Inputs:\n",
    "    x_ (tf.placeholder) : Input tensor\n",
    "    kernels1 (1D array) : Size of the encoder kernels (assumed square kernels)\n",
    "    kernels2 (1D array) : Size of the decoder kernels (assumed square kernels)\n",
    "    filters1 (1D array) : Number of filters in encoder layers\n",
    "    filters2 (1D array) : Number of filters in decoder layers\n",
    "    pool_size (1D array): Pooling size in each layer. Its length must be equal to len(kernels1)+len(kernels2)\n",
    "                          First len(kernels1) terms will be used as pooling layers of encoder/\n",
    "                          Remainin terms will be used as unpooling layers of decoder\n",
    "                          \n",
    "    Returns:\n",
    "    out_ (tf.placeholder)     : Output of the autoencoder without quantization in the middle\n",
    "    out_quant (tf.placeholder): Output of the autoencoder with quantization in the middle\n",
    "    '''\n",
    "    with tf.variable_scope(name):\n",
    "        out_=x_\n",
    "        for k in range(len(kernels1)):\n",
    "            conv = tf.layers.conv2d(inputs=out_,\n",
    "                                    filters=filters1[k],\n",
    "                                    kernel_size=[kernels1[k],kernels1[k]],\n",
    "                                    padding=\"same\",\n",
    "                                    activation=tf.nn.relu,\n",
    "                                    name='conv'+str(k))\n",
    "            pool_now=pool_size[k]\n",
    "            if(pool_now==1):\n",
    "                out_=conv\n",
    "            else:\n",
    "                out_ = tf.layers.max_pooling2d(inputs=conv, \n",
    "                                               pool_size=[pool_now,pool_now], \n",
    "                                               strides=pool_now,\n",
    "                                               name = 'pool'+str(k))\n",
    "\n",
    "            out_quant=tf.round(out_*255.)/255.\n",
    "\n",
    "        for k in range(len(kernels2)):\n",
    "            with tf.variable_scope(\"deconv\") as var_scope:\n",
    "                pool_now=pool_size[k+len(kernels1)]\n",
    "                if(pool_now==1):\n",
    "                    x_up=out_\n",
    "                    out_ = tf.layers.conv2d(inputs=x_up,\n",
    "                                            filters=filters2[k],\n",
    "                                            kernel_size=[kernels2[k],kernels2[k]],\n",
    "                                            padding=\"same\",\n",
    "                                            activation=tf.nn.relu,\n",
    "                                            name='deconv'+str(k))\n",
    "                    var_scope.reuse_variables() \n",
    "                    x_quant_up=out_quant\n",
    "                    out_quant = tf.layers.conv2d(inputs=x_quant_up,\n",
    "                                                filters=filters2[k],\n",
    "                                                kernel_size=[kernels2[k],kernels2[k]],\n",
    "                                                padding=\"same\",\n",
    "                                                activation=tf.nn.relu,\n",
    "                                                name='deconv'+str(k))\n",
    "                else:\n",
    "                    sh = out_.get_shape().as_list()\n",
    "                    x_up=tf.image.resize_images(out_,[sh[1]*pool_now,sh[2]*pool_now])\n",
    "                    out_ = tf.layers.conv2d(inputs=x_up,\n",
    "                                            filters=filters2[k],\n",
    "                                            kernel_size=[kernels2[k],kernels2[k]],\n",
    "                                            padding=\"same\",\n",
    "                                            activation=tf.nn.relu,\n",
    "                                            name='deconv'+str(k))\n",
    "                    var_scope.reuse_variables() \n",
    "                    x_quant_up=tf.image.resize_images(out_quant,[sh[1]*pool_now,sh[2]*pool_now])\n",
    "                    out_quant = tf.layers.conv2d(inputs=x_quant_up,\n",
    "                                                filters=filters2[k],\n",
    "                                                kernel_size=[kernels2[k],kernels2[k]],\n",
    "                                                padding=\"same\",\n",
    "                                                activation=tf.nn.relu,\n",
    "                                                name='deconv'+str(k))\n",
    "\n",
    "\n",
    "    return out_,out_quant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Working\n",
    "def apply_classification_loss_mse_with_rnn(kernels1=[5,7],kernels2=[7,5],\n",
    "                                     filters1=[16,128],filters2=[128,3],\n",
    "                                    pool_size=[2,2],learning_rate=1.,FT=False,depth =3):\n",
    "    '''\n",
    "    MSE based autoencoder optimizer.\n",
    "    \n",
    "    Inputs:\n",
    "    kernels1 (1D array) : Size of the encoder kernels (assumed square kernels)\n",
    "    kernels2 (1D array) : Size of the decoder kernels (assumed square kernels)\n",
    "    filters1 (1D array) : Number of filters in encoder layers\n",
    "    filters2 (1D array) : Number of filters in decoder layers\n",
    "    pool_size (1D array): Pooling size in each layer. Its length must be equal to len(kernels1)+len(kernels2)\n",
    "                          First len(kernels1) terms will be used as pooling layers of encoder/\n",
    "                          Remainin terms will be used as unpooling layers of decoder\n",
    "    learning_rate(float): Learning rate of the optimizer\n",
    "    FT (boolean)        : Boolean value for fine-tuning operation on decoder weights\n",
    "    depth(integer)      \n",
    "    \n",
    "    Returns:\n",
    "    model_dict          : Dictionary of the required output files\n",
    "    '''\n",
    "    with tf.Graph().as_default() as g:\n",
    "        with tf.device(\"/gpu:0\"):  # use gpu:0 if on GPU\n",
    "            x_ = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "            \n",
    "            \n",
    "            (x_out2,x_out_quant)=cnn_autoencoder(x_,pool_size=pool_size,kernels1=kernels1,filters1=filters1,\n",
    "                                kernels2=kernels2,filters2=filters2,name='filter0')\n",
    "            x_out1=x_\n",
    "            mse_loss1=tf.reduce_mean(tf.subtract(x_out1,x_out2)**2)\n",
    "            for k in range(1,depth):\n",
    "                (x_out3,x_out_quant)=cnn_autoencoder(x_out1-x_out2,pool_size=pool_size,kernels1=kernels1,filters1=filters1,\n",
    "                                kernels2=kernels2,filters2=filters2,name='filter'+str(k))\n",
    "                mse_loss1=tf.add(mse_loss1,tf.reduce_mean(tf.subtract(x_out1,tf.add(x_out2,x_out3))**2))\n",
    "                x_out1=x_out2\n",
    "                x_out2=x_out3\n",
    "            x_out3=x_out2\n",
    "            #y_dict = dict(labels=y_, logits=y_logits)\n",
    "            #losses = tf.nn.sparse_softmax_cross_entropy_with_logits(**y_dict)\n",
    "            #cross_entropy_loss = tf.reduce_mean(losses)\n",
    "            #mse_loss1=tf.reduce_mean(tf.subtract(x_,x_out)**2)\n",
    "            #a=tf.pad(tf.subtract(x_,x_out),[[0,0],[16,16],[16,16],[0,0]],'CONSTANT')\n",
    "\n",
    "            #mse_loss1=tf.reduce_mean(tf.nn.conv2d(a,h3,strides=[1,1,1,1],padding=\"VALID\")**2)\n",
    "            mse_loss2=tf.reduce_mean(tf.subtract(x_,x_out3)**2)\n",
    "            trainer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            if(FT):\n",
    "                with tf.variable_scope('deconv', reuse=True) as vs:\n",
    "                    var_list=[v for v in tf.global_variables() if v.name.startswith(vs.name)]\n",
    "                train_op = trainer.minimize(mse_loss1,var_list=var_list)\n",
    "            else:\n",
    "                train_op = trainer.minimize(mse_loss1)\n",
    "\n",
    "            #y_pred = tf.argmax(tf.nn.softmax(y_logits), dimension=1)\n",
    "            #correct_prediction = tf.equal(tf.cast(y_pred, tf.int32), y_)\n",
    "            #accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    model_dict = {'graph': g, 'inputs': x_,'outputs':x_out_quant, 'train_op': train_op, 'loss1': mse_loss1,'loss2': mse_loss2}\n",
    "    \n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_model(model_dict, dataset_generators, train_every=100, test_every=200, load=False,\n",
    "                learning_rate=1.,fname='cifar10_recon',outname='/tmp/cnn_autoencoder',ftname='/tmp/cnn_autoencoder'):\n",
    "    '''\n",
    "    Inputs:\n",
    "    model_dict: Output of apply_classification_loss_mse\n",
    "    x_tr      : Training images\n",
    "    x_test    : Test Images\n",
    "    x_32      : 32x32 patches of a big image\n",
    "    load      : Boolean for loading the weights from pre-trained network\n",
    "    fname     : Directory to save outputs\n",
    "    outname   : Directory to save (load=False) or load (load=True) weights\n",
    "    ftname    : Directory to save new weights when load+True\n",
    "    '''\n",
    "    with model_dict['graph'].as_default(), tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        ### WORK ON IT NOT TOO HARD\n",
    "        saver=tf.train.Saver()\n",
    "        if(load):\n",
    "            saver.restore(sess, outname)\n",
    "            print(\"Model loaded\")\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        ids=[i for i in range(100)]\n",
    "        for iter_i in range(10000):\n",
    "            batch_xs = x_tr[ids,:,:,:] \n",
    "            ids=[(ids[0]+100+i)%x_tr.shape[0] for i in range(100)]\n",
    "            sess.run(model_dict['train_op'], feed_dict={model_dict['inputs']: batch_xs})\n",
    "            \n",
    "            # test trained model\n",
    "            if iter_i % train_every == 0:\n",
    "                tf_feed_dict = {model_dict['inputs']: batch_xs}\n",
    "                loss_val = sess.run(model_dict['loss1'], feed_dict={model_dict['inputs']: batch_xs})\n",
    "                print('iteration %d\\t train mse: %.3E\\t'%(iter_i,loss_val))\n",
    "                if iter_i % test_every == 0:\n",
    "                    #tf_feed_dict = {x_: x_test}\n",
    "                    loss_val1 = sess.run(model_dict['loss1'], feed_dict={model_dict['inputs']: x_test})\n",
    "                    loss_val2 = sess.run(model_dict['loss2'], feed_dict={model_dict['inputs']: x_test})\n",
    "                    print('iteration %d\\t TEST MSE: %.3E\\t TEST MSE(Quantized): %.3E\\t'%(iter_i,loss_val1,loss_val2))\n",
    "                    \n",
    "                    \n",
    "                    lena_block=sess.run(model_dict['outputs'], \n",
    "                                       feed_dict={model_dict['inputs']:img_32})\n",
    "                    x_from_test=sess.run(model_dict['outputs'], \n",
    "                                         feed_dict={model_dict['inputs']:x_test[:5,:,:,:].reshape([-1,32,32,3])})\n",
    "                    \n",
    "                    lena_recon=block2img(lena_block,(512,512))\n",
    "                    lena_recon = convert2uint8(lena_recon*255.)\n",
    "                    skimage.io.imsave('../'+fname+'/lena4_'+str(int(iter_i/test_every))+'.tiff',lena_recon)\n",
    "\n",
    "                    for i in range(2):\n",
    "                        img_recon=convert2uint8((255*x_from_test[i,:,:,:]).reshape([32,32,3])).astype(np.uint8)\n",
    "                        skimage.io.imsave('../'+fname+'/test'+str(i)+'_'+str(int(iter_i/test_every))+'.tiff',img_recon)\n",
    "                        \n",
    "        saver = tf.train.Saver()\n",
    "        if(load):\n",
    "            outname=ftname\n",
    "        save_path = saver.save(sess, outname)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "        return saver\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CNN-RNN-AE with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\t train mse: 4.982E-01\t\n",
      "iteration 0\t TEST MSE: 5.656E-01\t TEST MSE(Quantized): 2.967E-01\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtezcan/anaconda3/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: ../cifar10_recon0/lena4_0.tiff is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/mtezcan/anaconda3/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: ../cifar10_recon0/test0_0.tiff is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/mtezcan/anaconda3/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: ../cifar10_recon0/test1_0.tiff is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100\t train mse: 2.486E-01\t\n",
      "iteration 200\t train mse: 1.516E-01\t\n"
     ]
    }
   ],
   "source": [
    "#YOU NEED TO CREATE A FOLDER NAMED 'cifar10_recon3' BEFORE RUNNING THAT CODE\n",
    "tf.reset_default_graph()\n",
    "model_dict=apply_classification_loss_mse_with_rnn(kernels1=[5,7,9,9],kernels2=[9,7,7,5],\n",
    "                                         filters1=[128,64,16,4],filters2=[8,8,3,3],\n",
    "                                         pool_size=[1,2,2,1,1,2,2,1],learning_rate=7e-5)\n",
    "saver = train_model(model_dict, [], train_every=100, test_every=2000,load=False,\n",
    "                    fname='cifar10_recon3',outname='/tmp/cnnx4_test2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
